{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nteZ2LF1gP_T"
      },
      "source": [
        "**FakeNewsNet Cleaning techniques**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75LjgLv2a15E"
      },
      "source": [
        "**Dataset 1: PolitiFact**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning Strategy for PolitiFact Dataset\n",
        "\n",
        "This section outlines the comprehensive cleaning techniques applied to the PolitiFact dataset:\n",
        "\n",
        "1. **Deduplication**: Removes duplicate articles/posts to prevent data leakage\n",
        "2. **Missing values/nulls**: Eliminates rows with missing critical data\n",
        "3. **Lowercasing**: Standardizes text for consistent tokenization\n",
        "4. **URL/user mention removal**: Cleans Twitter metadata (e.g., \"@user\", \"http://\")\n",
        "5. **Punctuation removal**: Eliminates noise from text\n",
        "6. **Emoji/HTML tag stripping**: Removes irrelevant or encoded characters\n",
        "7. **Non-English removal**: Filters to keep only English-language content using language detection\n",
        "8. **Data Imbalancing Check**: Verifies class distribution (fake vs. real) is acceptable for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Upload PolitiFact Datasets\n",
        "\n",
        "This cell uploads the PolitiFact fake and real news CSV files from your local machine to Google Colab for processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "laeibZ0-3477",
        "outputId": "6b7cb85c-65b1-482f-b7ee-9bd8e5df620d"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload files politifact_real and politifact_fake datasets\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load and Combine Datasets\n",
        "\n",
        "This cell performs the initial data loading and labeling:\n",
        "- Loads separate `politifact_fake.csv` and `politifact_real.csv` files\n",
        "- Adds a `label` column to each dataset ('fake' or 'real')\n",
        "- Combines both datasets into a single DataFrame for unified processing\n",
        "- Displays the first few rows to verify the structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "yarImtnbKH4q",
        "outputId": "0ebaf397-3c64-40c5-841b-3e1365f6929e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_fake = pd.read_csv(\"politifact_fake.csv\")\n",
        "df_real = pd.read_csv(\"politifact_real.csv\")\n",
        "\n",
        "df_fake['label'] = 'fake'\n",
        "df_real['label'] = 'real'\n",
        "\n",
        "df = pd.concat([df_fake, df_real], ignore_index=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Remove Duplicates and Handle Missing Values\n",
        "\n",
        "This cell performs initial data quality checks:\n",
        "- **Deduplication**: Identifies and removes duplicate entries based on the `title` column\n",
        "- **Missing value analysis**: Displays count of null values per column\n",
        "- **Essential field filtering**: Drops rows missing `title` or `label` (critical for modeling)\n",
        "\n",
        "These steps ensure data quality and prevent training issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkBpFojYKV1J"
      },
      "outputs": [],
      "source": [
        "# Check for duplicates in title (most common text field for detection)\n",
        "print(\"Duplicates in title:\", df.duplicated(subset='title').sum())\n",
        "\n",
        "# Remove duplicates by title\n",
        "df = df.drop_duplicates(subset='title')\n",
        "\n",
        "# Check for nulls\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Drop rows with missing title or label (essential for modeling)\n",
        "df = df.dropna(subset=['title', 'label'])\n",
        "\n",
        "# Preview cleaned structure\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Text Cleaning Function\n",
        "\n",
        "This cell defines and applies a comprehensive text cleaning function that:\n",
        "- **Lowercases** all text for consistency\n",
        "- **Removes URLs** (http/https links)\n",
        "- **Removes user mentions** (@username patterns)\n",
        "- **Removes punctuation** (special characters)\n",
        "- **Removes emojis and symbols** (Unicode characters)\n",
        "- **Strips whitespace** from beginning and end\n",
        "\n",
        "The cleaned text is stored in a new `clean_title` column, preserving the original for reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hANHiHUMM56Q"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()                          # Lowercase\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)               # Remove URLs\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)                  # Remove mentions\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)               # Remove punctuation\n",
        "    text = re.sub(r\"[\\u263a-\\U0001f645]\", \"\", text)   # Remove emojis/symbols\n",
        "    return text.strip()\n",
        "\n",
        "# Apply to title column\n",
        "df['clean_title'] = df['title'].apply(clean_text)\n",
        "\n",
        "# Preview result\n",
        "df[['title', 'clean_title', 'label']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Install Language Detection Library\n",
        "\n",
        "Installs the `langdetect` library, which will be used to identify and filter non-English text in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeYuVML_NaTF"
      },
      "outputs": [],
      "source": [
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Language Detection and Filtering\n",
        "\n",
        "This cell filters the dataset to include only English-language content:\n",
        "- Defines a safe language detection function that handles exceptions\n",
        "- Applies language detection to all cleaned titles\n",
        "- Filters to keep only rows where language is detected as English ('en')\n",
        "- Removes the temporary `language` helper column\n",
        "\n",
        "This ensures the model trains on consistent English text, improving performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L5cFr1-PBbz"
      },
      "outputs": [],
      "source": [
        "from langdetect import detect\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "\n",
        "# Function to safely detect language\n",
        "def detect_lang(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except LangDetectException:\n",
        "        return \"unknown\"\n",
        "\n",
        "# Apply to cleaned titles\n",
        "df['language'] = df['clean_title'].apply(detect_lang)\n",
        "\n",
        "# Filter only English rows\n",
        "df = df[df['language'] == 'en']\n",
        "\n",
        "# Drop helper column\n",
        "df = df.drop(columns=['language'])\n",
        "\n",
        "# Preview\n",
        "df[['clean_title', 'label']].sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Class Distribution Analysis\n",
        "\n",
        "This cell visualizes the balance between fake and real news:\n",
        "- Counts the number of samples for each label\n",
        "- Creates a bar chart showing the distribution\n",
        "- Helps identify if class imbalance exists\n",
        "\n",
        "A balanced dataset (or acceptable imbalance) is important for training unbiased classification models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPo4XVEPPQ3N"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count fake vs real\n",
        "class_counts = df['label'].value_counts()\n",
        "print(\"Class Distribution:\\n\", class_counts)\n",
        "\n",
        "# Plot bar chart\n",
        "class_counts.plot(kind='bar', color=['red', 'green'])\n",
        "plt.title(\"Class Distribution After Cleaning\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Save Cleaned PolitiFact Dataset\n",
        "\n",
        "This cell exports the cleaned dataset:\n",
        "- Saves only the essential columns (`clean_title` and `label`) to CSV\n",
        "- Downloads the file to your local machine for use in model training\n",
        "\n",
        "The cleaned dataset is now ready for machine learning pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSj9Ej-rP62q"
      },
      "outputs": [],
      "source": [
        "# Save to CSV\n",
        "df[['clean_title', 'label']].to_csv('clean_fakenewsnet.csv', index=False)\n",
        "\n",
        "# Download locally\n",
        "from google.colab import files\n",
        "files.download('clean_fakenewsnet.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxwScCkYa6Me"
      },
      "source": [
        "**Dataset 2: GossipCop**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning Strategy for GossipCop Dataset\n",
        "\n",
        "This section applies the same comprehensive cleaning techniques to the GossipCop dataset:\n",
        "\n",
        "1. **Deduplication**: Removes duplicate articles/posts\n",
        "2. **Missing values/nulls**: Eliminates rows with missing critical data\n",
        "3. **Lowercasing**: Standardizes text for consistent tokenization\n",
        "4. **URL/user mention removal**: Cleans Twitter metadata\n",
        "5. **Punctuation removal**: Eliminates noise from text\n",
        "6. **Emoji/HTML tag stripping**: Removes irrelevant characters\n",
        "7. **Non-English removal**: Filters to keep only English content\n",
        "8. **Data Imbalancing**: Addresses class imbalance using undersampling if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Upload GossipCop Datasets\n",
        "\n",
        "This cell uploads the GossipCop fake and real news CSV files from your local machine to Google Colab for processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jedgyxTQkbL"
      },
      "outputs": [],
      "source": [
        "# Upload files gossipcop_real and gossipcop_fake datasets\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Load and Combine GossipCop Datasets\n",
        "\n",
        "This cell performs initial data loading for GossipCop:\n",
        "- Loads separate `gossipcop_fake.csv` and `gossipcop_real.csv` files\n",
        "- Adds a `label` column to each dataset ('fake' or 'real')\n",
        "- Combines both datasets into a single DataFrame\n",
        "- Displays the first few rows to verify the structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIG8HGWHbz9E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load files\n",
        "df_fake = pd.read_csv(\"gossipcop_fake.csv\")\n",
        "df_real = pd.read_csv(\"gossipcop_real.csv\")\n",
        "\n",
        "# Label the data\n",
        "df_fake['label'] = 'fake'\n",
        "df_real['label'] = 'real'\n",
        "\n",
        "# Combine\n",
        "df = pd.concat([df_fake, df_real], ignore_index=True)\n",
        "\n",
        "# Preview\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Remove Duplicates and Handle Missing Values (GossipCop)\n",
        "\n",
        "This cell performs data quality checks for GossipCop:\n",
        "- **Deduplication**: Identifies and removes duplicate entries based on `title`\n",
        "- **Missing value analysis**: Displays count of null values per column\n",
        "- **Essential field filtering**: Drops rows missing `title` or `label`\n",
        "\n",
        "Same cleaning approach as PolitiFact to ensure consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4NPuAlZcNd4"
      },
      "outputs": [],
      "source": [
        "# Check for duplicate titles\n",
        "print(\"Duplicates in title:\", df.duplicated(subset='title').sum())\n",
        "\n",
        "# Remove duplicate titles\n",
        "df = df.drop_duplicates(subset='title')\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Drop rows missing title or label (core for modeling)\n",
        "df = df.dropna(subset=['title', 'label'])\n",
        "\n",
        "# Preview cleaned structure\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Text Cleaning (GossipCop)\n",
        "\n",
        "This cell applies the same comprehensive text cleaning function to GossipCop:\n",
        "- Lowercases text\n",
        "- Removes URLs, user mentions, punctuation\n",
        "- Removes emojis and symbols\n",
        "- Strips whitespace\n",
        "\n",
        "Creates a `clean_title` column with the processed text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZk56UI6cRO4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Define clean text function\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()  # Lowercase\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)     # Remove mentions\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
        "    text = re.sub(r\"[\\u263a-\\U0001f645]\", \"\", text)  # Remove emojis/symbols\n",
        "    return text.strip()\n",
        "\n",
        "# Apply to title column\n",
        "df['clean_title'] = df['title'].apply(clean_text)\n",
        "\n",
        "# Preview result\n",
        "df[['title', 'clean_title', 'label']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 13: Language Detection and Filtering (GossipCop)\n",
        "\n",
        "This cell filters GossipCop to include only English-language content:\n",
        "- Applies safe language detection to all cleaned titles\n",
        "- Filters to keep only English ('en') content\n",
        "- Removes the temporary `language` helper column\n",
        "\n",
        "Ensures consistency with PolitiFact dataset processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox22fF3mc6aa"
      },
      "outputs": [],
      "source": [
        "from langdetect import detect\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "\n",
        "# Define function for language detection\n",
        "def detect_lang(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except LangDetectException:\n",
        "        return \"unknown\"\n",
        "\n",
        "# Apply to cleaned titles\n",
        "df['language'] = df['clean_title'].apply(detect_lang)\n",
        "\n",
        "# Keep only English\n",
        "df = df[df['language'] == 'en']\n",
        "\n",
        "# Drop helper column\n",
        "df = df.drop(columns=['language'])\n",
        "\n",
        "# Preview result\n",
        "df[['clean_title', 'label']].sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 14: Class Distribution Analysis (GossipCop)\n",
        "\n",
        "This cell visualizes the balance between fake and real news in GossipCop:\n",
        "- Counts samples for each label\n",
        "- Creates a bar chart showing the distribution\n",
        "- Identifies if class imbalance needs to be addressed\n",
        "\n",
        "GossipCop often has significant class imbalance that requires correction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ngJAeiQdK_Y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count labels\n",
        "class_counts = df['label'].value_counts()\n",
        "print(\"\\nClass Distribution:\\n\", class_counts)\n",
        "\n",
        "# Plot distribution\n",
        "class_counts.plot(kind='bar', color=['red', 'green'])\n",
        "plt.title(\"Class Distribution After Cleaning\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 15: Address Class Imbalance with Random Undersampling\n",
        "\n",
        "This cell balances the GossipCop dataset using random undersampling:\n",
        "- **Reference**: Based on \"A comprehensive survey of fake news in social networks: Attributes, features, and detection approaches\"\n",
        "- **Method**: Random undersampling of the majority class\n",
        "- **Process**:\n",
        "  - Separates data by class (fake vs. real)\n",
        "  - Downsamples the majority class (real news) to match minority class size\n",
        "  - Combines and shuffles the balanced dataset\n",
        "  - Verifies equal class distribution\n",
        "\n",
        "This prevents model bias toward the majority class during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpO8kVnMfSSx"
      },
      "outputs": [],
      "source": [
        "# Reference for Undersampling technique: A comprehensive survey of fake news in social networks: Attributes,features, and detection approaches (random undersampling technique)\n",
        "\n",
        "# Separate by class\n",
        "df_real = df[df['label'] == 'real']\n",
        "df_fake = df[df['label'] == 'fake']\n",
        "\n",
        "# Downsample real news\n",
        "df_real_downsampled = df_real.sample(n=len(df_fake), random_state=42)\n",
        "\n",
        "# Combine and shuffle\n",
        "df_balanced = pd.concat([df_fake, df_real_downsampled], ignore_index=True)\n",
        "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Confirm balance\n",
        "print(df_balanced['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 16: Save Cleaned GossipCop Dataset\n",
        "\n",
        "This cell exports the cleaned and balanced GossipCop dataset:\n",
        "- Saves only essential columns (`clean_title` and `label`) to CSV\n",
        "- Downloads the file to your local machine\n",
        "\n",
        "The cleaned GossipCop dataset is now ready for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmhNemFOfdP_"
      },
      "outputs": [],
      "source": [
        "# Save to CSV\n",
        "df_balanced[['clean_title', 'label']].to_csv('clean_gossipcop.csv', index=False)\n",
        "\n",
        "# Download locally (Colab)\n",
        "from google.colab import files\n",
        "files.download('clean_gossipcop.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
